{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy import stats\n",
    "from scipy.stats import t\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        64\n",
       "1       132\n",
       "2        66\n",
       "3       190\n",
       "4       167\n",
       "       ... \n",
       "1012      0\n",
       "1013    190\n",
       "1014    136\n",
       "1015    153\n",
       "1016    136\n",
       "Name: 1->2Bytes, Length: 1017, dtype: int64"
      ]
     },
     "execution_count": 436,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_preds = {}\n",
    "# load in data here\n",
    "df1 = pd.read_csv(\"imnemato-youtube[1080p]-1x-vpn-mac-clean-20201031a.csv\")\n",
    "df2 = pd.read_csv(\"vpn_browsing1.csv\")\n",
    "\n",
    "df3 = pd.read_csv(\"pgaddiso-youtube-1x-720p-vpn-linux-clean-20201102-1.csv\")\n",
    "df4 = pd.read_csv(\"vpn_browsing2.csv\")\n",
    "\n",
    "\n",
    "df5 = windows_video_vpn = pd.read_csv(\"stdoan-youtube[720p60]-1x-vpn-windows-clean-20201102.csv\")\n",
    "df6 = pd.read_csv(\"vpn_browsing3.csv\")\n",
    "\n",
    "# make sure that dataframes are in list\n",
    "list_of_frames = [df1, df2, df3,df4,df5,df6]\n",
    "#add vpn dataframes to next list\n",
    "video_frames = [df1,df3,df5]\n",
    "browsing_frames = [df2,df4,df6]\n",
    "\n",
    "browsing_frames[1][\"1->2Bytes\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data cleaning\n",
    "# helper method to sum all package lengths for each entry in a column\n",
    "def package(input1):\n",
    "    sum1 = 0\n",
    "    for x in input1:\n",
    "        if x  != \"\":\n",
    "            sum1 += int(x)\n",
    "    return sum1\n",
    "\n",
    "# adds total pacakge length to all dataframes\n",
    "def packet_length(frames):\n",
    "    for j in frames:\n",
    "        j[\"sum of sizes\"] = pd.Series(j.packet_sizes.apply(lambda x: x.split(\";\"))).apply(lambda x: package(x))\n",
    "\n",
    "#label the data\n",
    "def label():\n",
    "    for x in video_frames:\n",
    "            x[\"Label\"] = 1\n",
    "    for j in browsing_frames:\n",
    "            j[\"Label\"] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labes data \n",
    "label()\n",
    "\n",
    "# total packet length\n",
    "packet_length(list_of_frames)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ml_model_data(frames):\n",
    "    # global_varaiables for data storage\n",
    "    X_train_df =  None\n",
    "    X_test_df  = None\n",
    "    y_train_df = None\n",
    "    y_test_df = None\n",
    "\n",
    "    for f in frames:\n",
    "        #isolate features in frames\n",
    "        X = f[[\"sum of sizes\", \"1->2Bytes\",\"2->1Bytes\"]]\n",
    "        \n",
    "        #isolate label\n",
    "        y = f[[\"Label\"]]\n",
    "        \n",
    "        #split data between training and test\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3) \n",
    "       \n",
    "        #gatther data from all dataframes\n",
    "        X_train_df = pd.concat([X_train_df, X_train])\n",
    "        X_test_df = pd.concat([X_test_df, X_test])\n",
    "        y_train_df = pd.concat([y_train_df, y_train])\n",
    "        y_test_df = pd.concat([y_test_df, y_test])\n",
    "        \n",
    "        \n",
    "        \n",
    "    return [X_train_df,X_test_df,y_train_df,y_test_df]\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [],
   "source": [
    "All_data = ml_model_data(list_of_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_predictions(ml_model_data):\n",
    "    clf = LogisticRegression()\n",
    "    clf.fit(ml_model_data[0], ml_model_data[2])\n",
    "    predictions = clf.predict(ml_model_data[1]) \n",
    "    return predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_predictions(ml_model_data):\n",
    "    clf = svm.SVC()\n",
    "    clf.fit(ml_model_data[0], ml_model_data[2])\n",
    "    predictions = clf.predict(ml_model_data[1])\n",
    "    return predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bayes_predictions(ml_model_data):\n",
    "    clf =  GaussianNB()\n",
    "    clf.fit(ml_model_data[0], ml_model_data[2])\n",
    "    predictions = clf.predict(ml_model_data[1])\n",
    "    return predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_predictions(ml_model_data):\n",
    "    clf = KNeighborsClassifier(n_neighbors=3)\n",
    "    clf.fit(ml_model_data[0], ml_model_data[2])\n",
    "    predictions = clf.predict(ml_model_data[1])\n",
    "    return predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dania\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "k_preds = k_predictions(All_data)\n",
    "all_preds[\"k\"] = k_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dania\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "bayes_preds = bayes_predictions(All_data)\n",
    "all_preds[\"bayes\"] = bayes_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dania\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\dania\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "svm_preds = svm_predictions(All_data)\n",
    "all_preds[\"svm\"] = svm_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dania\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\dania\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "predictions = logistic_predictions(All_data)\n",
    "all_preds[\"LogisticRegression\"] = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confidence_interval(prediction, confidence):\n",
    "    min_value = min(prediction)\n",
    "    max_value = max(prediction)\n",
    "    \n",
    "    average_list = []\n",
    "    \n",
    "    for x in range(1000):\n",
    "        samples = np.random.choice(prediction, len(prediction))\n",
    "        average_list.append(np.mean(samples))\n",
    "    \n",
    "    size = len(average_list)\n",
    "    average = np.mean(average_list)\n",
    "    st_error = stats.sem(average_list)\n",
    "    interval = st_error * t.ppf((1 + confidence)/2,size-1)\n",
    "    low = round(average- interval)\n",
    "    up = round(average +interval)\n",
    "    return (low, up)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k 0.8604155567394779 correct\n",
      "bayes 0.7384123601491742 correct\n",
      "svm 0.8614810868407032 correct\n",
      "LogisticRegression 0.2749067661161428 correct\n"
     ]
    }
   ],
   "source": [
    "for x in all_preds:\n",
    "    percent_correct = sum(np.array(All_data[3][\"Label\"]) == np.array(all_preds[x]))/len(All_data[3][\"Label\"])\n",
    "    print(str(x) +\" \"+ str(percent_correct) + \" correct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean squared error of k 0.1395844432605221\n",
      "The mean squared error of bayes 0.2615876398508258\n",
      "The mean squared error of svm 0.13851891315929676\n",
      "The mean squared error of LogisticRegression 0.7250932338838573\n"
     ]
    }
   ],
   "source": [
    "for x in all_preds:\n",
    "    print(\"The mean squared error of \"+str(x) + \" \"+ str(mean_squared_error(All_data[3][\"Label\"], all_preds[x])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The best Model is K means, now time to see what features we can change in model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ml_model_data1(frames):\n",
    "    # global_varaiables for data storage\n",
    "    X_train_df =  None\n",
    "    X_test_df  = None\n",
    "    y_train_df = None\n",
    "    y_test_df = None\n",
    "\n",
    "    for f in frames:\n",
    "        #isolate features in frames\n",
    "        X = f[[\"sum of sizes\", \"1->2Bytes\", \"Port1\"]]\n",
    "        \n",
    "        #isolate label\n",
    "        y = f[[\"Label\"]]\n",
    "        \n",
    "        #split data between training and test\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3) \n",
    "       \n",
    "        #gatther data from all dataframes\n",
    "        X_train_df = pd.concat([X_train_df, X_train])\n",
    "        X_test_df = pd.concat([X_test_df, X_test])\n",
    "        y_train_df = pd.concat([y_train_df, y_train])\n",
    "        y_test_df = pd.concat([y_test_df, y_test])\n",
    "        \n",
    "        \n",
    "        \n",
    "    return [X_train_df,X_test_df,y_train_df,y_test_df]\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_predictions1(ml_model_data):\n",
    "    clf = KNeighborsClassifier(n_neighbors=3)\n",
    "    clf.fit(ml_model_data[0], ml_model_data[2])\n",
    "    predictions = clf.predict(ml_model_data[1])\n",
    "    return predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dania\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9057005860415557"
      ]
     },
     "execution_count": 429,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "All_data = ml_model_data1(list_of_frames)\n",
    "new_k = k_predictions1(All_data)\n",
    "percent_correct = sum(np.array(All_data[3][\"Label\"]) == np.array(new_k))/len(All_data[3][\"Label\"])\n",
    "percent_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [],
   "source": [
    "tn, fp, fn, tp = confusion_matrix(All_data[3][\"Label\"], new_k).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The true negatives 1285\n",
      "The true positives 415\n",
      "The False positives 76\n",
      "The False negatives 101\n"
     ]
    }
   ],
   "source": [
    "print(\"The true negatives \" + str(tn))\n",
    "print(\"The true positives \" + str(tp))\n",
    "print(\"The False positives \" + str(fp))\n",
    "print(\"The False negatives \" + str(fn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>153</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>252</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>362</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Label\n",
       "250      1\n",
       "153      1\n",
       "72       1\n",
       "252      1\n",
       "362      1"
      ]
     },
     "execution_count": 479,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "All_data[3][All_data[3][\"Label\"] != new_k].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, 1, 0, 1, 1, 1, 1, 0], dtype=int64)"
      ]
     },
     "execution_count": 480,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_k[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 481,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(All_data[3][\"Label\"][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I seem to have false positives at index 2 and 4, and 9.  Looking at training data  I get these False postives when there is sudden change in data. The data makes labels based on the data points near it. Since there is a change in the data it ruins the pattern having it classified wrong. Kmeans looks at the algorithim near every cluster hence data fluxations can mess up algorithm when training the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
